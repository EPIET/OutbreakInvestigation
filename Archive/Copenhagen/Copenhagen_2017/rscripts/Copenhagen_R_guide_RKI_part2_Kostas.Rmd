---
title: "Outbreak of gastroenteritis after a high school dinner in Copenhagen, Denmark, November 2006 - Part 2"
subtitle: "adapted from the UK FETP version"
author: "Johannes Boucsein"
date: "16. November, 2021"
output:
  html_document: 
      toc: yes
      toc_float:
        collapsed: yes
        smooth_scroll: yes
  pdf_document: 
      toc: true
      toc_depth: 3
urlcolor: blue
---

```{r setup, include = FALSE}
# set chunk WD
library(knitr)
opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # set wd to project folder

# load packages
pacman::p_load(char = c("here", "skimr", "janitor", "lubridate", "epiR", "apyramid", "tidyverse"), install = FALSE)
```


```{r restrict output, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}

################################################################
# FUNCTION TO RESTRICT RESULTS TO A FEW LINES OF OUTPUT:

# Check if knitr is installed, if not install it:
if (!requireNamespace("knitr", quietly = TRUE)) install.packages("knitr")

# Define empty output:
hook_output <- knit_hooks$get("output")

# Function to restrict lines of output:
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

``` 



This document is the compendium to the second part of the *Copenhagen* case study as taught at the ***EPIET*** *Outbreak Investigation*-module in December 2021. This document is not meant as a general introduction to **R** statistical computation software.  

# Copyright and license {-}
This case study was designed under a European Centre for Disease Prevention and Control (ECDC) service contract for the development of training material (2010). The data were slightly modified for training purposes.

The following code has been updated from the [UK FETP version](https://github.com/EPIET/OutbreakInvestigation) of the case study to modern *R* utilizing the [*tidyverse*](https://www.tidyverse.org/) and other contemporary packages. Some parts are adapted from [*The Epidemiologist R Handbook*](https://epirhandbook.com/en/). 

The initial contributors and copyright license are listed below. All copyrights and licenses of the original document apply here as well. 

**Previous authors and contributors to this R companion guide:** 

 - Amy Michail
 - Alexander Spina
 - Patrick Keating 
 - Daniel Gardiner
 - Lukas Richter
 - Ashley Sharp 
 - Hikaru Bolt 

**Source:**
This case study is based on an investigation conducted by Jurgita Pakalniskiene, Gerhard Falkenhorst (Statens Serum Institut, Copenhagen) and colleagues.

**Case study authors:**
Jurgita Pakalniskiene, Gerhard Falkenhorst, Esther Kissling, Gilles Desv

**Reviewers:**
Marta Valenciano, Alain Moren.

**Adaptions for previous modules:**

 - Irina Czogiel
 - Kristin Tolksdorf
 - Michaela Diercke
 - Sybille Somogyi
 - Christian Winter
 - Sandra Dudareva-Vizule
 - Katharina Alpers
 - Alicia Barrasa
 - Androulla Efstratiou
 - Steen Ethelberg
 - Annette HeiÃŸenhuber
 - Aftab Jasir
 - Ioannis Karagiannis
 - Pawel Stefanoff
 - Kostas Danis


\pagebreak 


**You are free:**

- to Share: copy, distribute and transmit the work
- to Remix: adapt the work, under the following conditions:

**Attribution:**

- You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). 
- The best way to do this is to keep as it is the list of contributors (sources, authors and reviewers).

**Share Alike:**

- If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. 
- Your changes must be documented. 
- Under that condition, you are allowed to add your name to the list of contributors.

**Use in teaching and training:**
You cannot sell this work alone but you can use it as part of a teaching programme, with the understanding that:

- Waiver: Any of the above conditions can be waived if you get permission from the copyright holder.
- Public Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.
- Other Rights - In no way are any of the following rights affected by the license:
    + Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations;
    + The author's moral rights;
    + Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.
- Notice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.

This licence is based on a Creative Commons 3.0 license (http://creativecommons.org/licenses/by-sa/3.0/)

\pagebreak 

## Loading the analysis data set:

Before anything, make sure that all necessary packages are loaded. Then: load the data. To ensure that everyone works with the same data, we provide a file. Please read in `Copenhagen_analysis_clean.rds` and check if all variables are as expected using `skim()`. 

Remember that each package usually has a detailed documentation available online and that you can get more information about particular functions by calling `?function()`, e.g. `?table()`.

```{r eval=FALSE}
# load packages
install.packages("pacman")
library(pacman)

pacman::p_load(
  devtools,
  tidyverse,
  here,
  lubridate,
  skimr,
  here,
  rio,
  janitor,
  lubridate, 
  epiR, 
  apyramid,
  gtsummary,
  EpiStats,
  fextable
)

```

```{r}
# Read in data
#If your data are in the "data" sub-forlder of your working folder, use `here("data",)` to provide the path.

cph <- rio::import(here("data", "Copenhagen_analysis_clean.rds")) 

```


# Recap of part 1 {-}
In part 1 of the *Copenhagen* case study, you read the survey data, cleaned the data, including variable formatting, checking for plausibility, and defining new variables, stored the clean data in an R-native file format, and had a first look at the results. 

  - Was there anything you struggled with and could not find a solution for yourself? 
  - Which `R` concepts where new to you?
  
# 1. Developing an analysis plan

In theory, you create an analysis plan before data collection - it is an important way to ensure you collect all the data you need and that you *only* collect the data you need (think: data protection!). However, during outbreak investigations, you may need to deploy questionnaires as soon as possible and before a plan has been developed. In these cases, your experience will be an important resource to fall back on. 

## Section goals
In this section, you will learn how to develop an analysis plan to help guide the descriptive and analytical analyses.

## Taskn 04: Develop an analysis plan and key dummy tables

In your analysis plan you should create a document to include:

 - research question(s) and hypothesis
 - dataset(s) to be used
 - inclusion / exclusion criteria for records you will use in your analysis
 - list of variables that will be used in the main analysis (main exposure, outcome, stratifying variables)
 - statistical methods
 - key mock-up tables (univariable, bivariable, stratified) including title, labels and expected format for results 

\pagebreak 


# 2. Descriptive analysis in `R` 

Before undertaking any further analytical steps, building models, or testing hypothesis it is extremely important to use descriptive statistics. Descriptive statistics help to 

  - understand your data 
  - identify leads and generate ideas that you might want to follow with statistical tests
  - check if your data fulfill the assumptions underlying the statistical models and tests you plan to use (for example: Are observations independent of each other? Do they follow a *normal* distribution?) - if your data do not fulfill these assumptions, you may need to change your analysis plan or transform the data (in data science slang, this step is called *feature engineering*)

## Section goals

In this section of the case study you will  

 - perform discriptive analysis of a dataset
 - be introduced to the concept of *iteration* 

## Task: Descriptive analysis

 - describe the data by person (age and sex, other attributes?)
 - create an age/sex-pyramid
 - describe the data by time (= construct an *epicurve*)
 - create a histogram of the incubation periods
 - calculate attack proportions
 - summarise the analysis so far and generate hypotheses:
     + Any surprising findings regarding cohort characteristics?
     + Is the shape of the epicurve in line with the hypothesis that the outbreak has a viral or toxic aetiology?
     + What other information can you obtain from the epicurve?
     + Does the summary of symptoms help you narrow down the suspected agent?
     + Do you have any hypotheses about the vehicle of infection so far?

\pagebreak

## Help for task 05 {.tabset .tabset-pills}


### Describe the data by person - `sex` 

In your homework assignment, we used `table()` from baseR to construct tables. This time, we are goint to introduce another function that produces good-looking tables and makes it hassle-free to add additional information that go beyond `table()` capabilities: `janitor::tabyl()`. 

```{r }
# single-table
# either notation is fine
tabyl(cph$case)
tabyl(cph, case)

# cross-table
tabyl(cph, case, sex)
```
So far, the only difference between `table()`and `tabyl()` is that `tabyl()` expects a dataset and variable names when cross-tabulating. This is convenient when working with the `tidyverse`, because it fits right in the `pipe (%>%)` structures: 

```{r}
cph %>% 
  tabyl(case, sex)
```

The power of `tabyl()`, however, shows when you want additional information that goes beyond counts: totals, percentages, and so on. For these steps, `tabyl()` comes with several *helper*-functions that follow a `adorn_XYZ()`-structure.

```{r}
cph %>% tabyl(diarrhoea, case) %>%
  # add additional info:
  # row and col totals      
  adorn_totals(where = "both") %>% 
  # turn into percentages      
  adorn_percentages() %>% 
  adorn_pct_formatting(digits = 1) %>% 
  # add counts, again
  adorn_ns(position = "front") %>% 
  # change titles
  adorn_title(col_name = "case", row_name = "diarrhoea") 
      
```

For more detail on `tabyl()`-function and the `adorn_XYZ`-helpers, see the [`janitor`-documentation](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html) or [*The Epidemiologist R Handbook* section on tabulation](https://epirhandbook.com/en/descriptive-tables.html#tbl_janitor).

You can now create a frequency table of several variable in the dataset, using the `tbl_summary` function from gtsummary package.

```{r}
#Use the `tbl_summary` function from gtsummary package
cph %>% 
  select(case, diarrhoea,bloody,vomiting,abdo,nausea,fever,headache,jointpain) %>% 
  gtsummary::tbl_summary()
```
```{r}
#Do a frequency table of all the variables by case status
cph %>% 
  select(case, diarrhoea,bloody,vomiting,abdo,nausea,fever,headache,jointpain) %>% 
  gtsummary::tbl_summary(by=case) %>% 
  add_p()
```

&nbsp;
&nbsp;

### Optional: introducing *iteration* and `purrr()`

Sooner or later, in any data analysis, you approach the point where you would like to perform a certain task repeatedly. In our case, for example, we would like to analyze not only the relationship between `diarrhoea`and `case`, but the relationship of *all* symptoms and `case`. One way to approach this task is to copy-paste the code we just created for cross-tabulation as many times as necessary and change the arguments. This approach, however, has several downsides:

  - We introduce the chance of making incidental mistakes when copying, pasting, and changing the arguments (= more errors)
  - If requirements change, we need to change the arguments and variable names in each and every instance (= more work)
  - Our code gets longer and may be harder to read (= less practical)
 
Of course, these points do not only apply to our specific example, but to all instances where you would like to perform a task repeatedly. Even more so, because our code, so far, has been quite simple. The more complex, the more likely youÂ´ll make errors, the more youÂ´ll have to change, and the harder it gets to understand whatÂ´s going on. 
 
A better approach for repeated performance of a task is ***iteration***. Iteration means repeating the same operation on different inputs, for example on different columns or on different datasets. In many programming languages, you would be using a technique called *looping*. While `R` also supports looping, `R` is a *functional programming* language and tackles iteration by focusing on *functions*. It might take you a while to wrap your head around the idea, but it is worth the investment. We will be using the `purrr` package, which provides functions that eliminate the need for many common for loops (note that the `apply` family of functions in base R (apply(), lapply(), tapply(), etc) solve a similar problem, but `purrr` is more consistent and thus is easier to learn). Once again, unfortunately, this guide can not be a comprehensive introduction to *functional programming* or `purrr`- however there are plenty of brilliant introductions and sources available for free, such as [the *purrr* webpage](https://purrr.tidyverse.org/), [*R for Data Science* chapter on *iteration*](https://r4ds.had.co.nz/iteration.html), and [*The Epidemiologist R Handbook* Chapter on *iteration*](https://epirhandbook.com/en/iteration-loops-and-lists.html).

For now, we will be using the `purrr::map()`. Call the help page to find out more about `map()`. Focus on:

  - Which inputs does `map()` expect?
  - Which output does it generate?

```{r eval = FALSE, echo = FALSE}
# including some javascript to hide spoiler
```

---

<div style="display: none;" id="hidden_text1">  
`map()` expects two inputs: 

 - `.x` = a `list` or vector
 - `.f` = a function to apply to the elements of `.x`
 
`map()` returns as `list`. So far, we have worked with `vectors` and `dataframes`. Luckily for us, `dataframes` are a special type of `list` and we will thus not need to pay too much attention to this new class of object for now. 
</div>
<button href="#" onclick="$('#hidden_text1').show(); return false;">Click here to show spoiler 
</button>

---

LetÂ´s give it a try: First, create `.x`, the input: Which variables do we want to have a look at?

```{r}
# Define the variables which you would like to tabulate 
vars_interested <- c("diarrhoea","bloody","vomiting","abdo","nausea","fever","headache","jointpain")

# create the dataframe we want to 'map()' on
tmp <- cph %>% 
  filter(case == TRUE) %>% 
  select(all_of(vars_interested))

# keep your environment clean, discard unused objects
rm(vars_interested)
```

What does `tmp` look like? Check, if `tmp` is a list and if it is a dataframe by calling `is.list()` and `is.data.frame()`. 

Now, lets think about `.f`, the function weÂ´d like to map to our dataframe/list: We are going to use the function we used for tabulation earlier: `tabyl()`, together with some `adorn_xxx()`-helper. 

```{r eval = FALSE}
# ItÂ´s going to look like this:
tabyl() %>%
  adorn_pct_formatting(digits = 1)
# Note that the function as is wonÂ´t run, because we have not given it any input yet!  
```

Now, put the pieces together: 

```{r, results='hide'}
map(.x = tmp,
    .f = ~ tabyl(.x) %>% adorn_pct_formatting(digits = 1))
```

What does the output look like? Note, that we preceed our `tabyl()` function with a tilde (~) and how we tell `tabyl()` which data to tabyl on by passing it the `.x`-argument. What happens, if you leave `.x` out? 


\pagebreak
### Describe the data by person - `age` 

Let us return to describing our dataset and focus on `age`

We can first have a look at the age range: 

```{r results="hide"}
#look at age 
skim(cph$age)
```
```{r}
# print a basic histogram of 'age'
hist(cph$age)
```
```{r}
#You can also use the ggplot function
ggplot(data=cph,
       mapping=aes(x=age))+
  geom_histogram()+
  theme_minimal()
```


To construct an age/sex pyramid, we are going to use the `age_pyramid()`-function from the `apyramid`-package. Call the function's help page to see which inputs it requires and which other arguments you may need to specify. 

`age_pyramid()` requires a dataframe, a grouping variable with age goups, and a grouping variable for the sexes. Remember: in `R`, categorical variables are expressed as `factors`. In `cph`, `sex` is already a factor variable, but `age` is an `integer` (=continuous) variable. We thus need to create a `factor` variable for the age groups first. You have different options to achieve this goal: use the `case-when()`-function introduced in part 1, or use `cut()`-functioin, that *cuts* continuous variables in groups and returns a `factor` variable. 

```{r}
# Create age group:
cph <- cph %>% 
  mutate(age_cat = cut(age, breaks = c(-Inf, 10, 16, 20, 50, Inf),
                        labels = c("0-10", "11-16", "17-20", "21-50",">50"),
                        right = TRUE,
                        ordered_result = TRUE)
         )
```

```{r, warning=FALSE, message=FALSE}
apyramid::age_pyramid(cph, 
                      age_group = age_cat,
                      split_by = sex) +
  # to adapt the scale of the plot to better fit our data:
  scale_y_continuous(limits = c(-150,150),
                     breaks = seq(from = -150, to = 150, by = 25),
                     labels = as.character(abs(seq(from = -150, to = 150, by = 25))))
```


### Create an epicurve

To create an *epicurve*, we are going to use `ggplot2`. GGplot2 is a versatile package that helps create beautiful visualizations in `R`. Unfortunately, it is not the most straight-forward package to learn and a comprehensive introduction to ggplot2 is well beyond the scope of this compendium. However, there are plenty good in-depth guides to the package, for example: [*The Epidemiologist R Handbook* Chapter on *ggplot*](https://epirhandbook.com/en/ggplot-basics.html) and [*ggplot2 - Elegant Graphics for Data Analysis*](https://ggplot2-book.org/index.html). 

```{r}
cph %>% 
  filter(case == TRUE) %>% 
  mutate(onset_day = factor(onset_day)) %>% 
  ggplot(aes(x = onset_day)) + 
  geom_bar() +
  # adapt scale for better readability
  scale_x_discrete(guide = guide_axis(angle = 90)) + 
  # apply a leaner esthetic to the graph
  theme_minimal()
```

As you can see, the epicurve is quite crude, because most cases had their disease onset on November 12th. To increase resolution, we could enhance the time information to include both, day and hour. The * day* information is stored in `onset_day`, the *hour* information is stored in `onset_hour` - letÂ´s create a new variable: `onset_dayhour`. First, however, letÂ´s make sure that we have complete information: everyone with an `onset_day` should also have an `onset_hour`. For this task, we use `any()` - call the help-function to learn more about it. 

```{r, results="hide"}
# First: Check if exist observations with dayonset but without starthour
# does anyone have onset_daay, but not onset_hour?
any(!is.na(cph$onset_day) & is.na(cph$onset_hour))
```
```{r}
# create new variable 'onset_dayhour'
cph <- cph %>% 
  mutate(onset_dayhour = onset_day + hours(onset_hour))
```

```{r}
cph %>% 
  filter(case == TRUE) %>% 
  mutate(onset_dayhour = factor(onset_dayhour)) %>% 
  ggplot(aes(x = onset_dayhour)) + 
  geom_bar() +
  # adapt scale for better readability
  scale_x_discrete(guide = guide_axis(angle = 90)) + 
  # apply a leaner esthetic to the graph
  theme_minimal()
```

Finally (and a bit to show off what ggplot offers) we could compare the epicurve between the sexes and additionally investigate how teachers vs students were distributed. Here, `fill`  adds an additional variable to be displayed in the plot: `group` is going to determine the fill-colour of our bars. `facet-Wrap` splits the graph in two: one each for the two levels of `sex`.

```{r}
cph %>% 
  filter(case == TRUE) %>% 
  ggplot(aes(x = factor(onset_dayhour), fill = factor(group))) + 
  geom_bar() +
  facet_wrap(vars(sex)) +
  # adapt scale for better readability
  scale_x_discrete(guide = guide_axis(angle = 90)) + 
  # change titles and labels
  labs(fill = "group",
       x = "day and hour of onset",
       y = "number of cases",
       title = "Epi-Curve of the outbreak, divided by sex",
       subtitle = "Copenhagen, November 2006, N(total) = 215 cases")
```

### Incubation period in hours

> The incubation period of a disease is the time from an exposure resulting in infection until the onset of disease. Because infection usually occurs immediately after exposure, the
incubation period is generally the duration of the period from the onset of infection to the onset of disease.
> 
> `r tufte::quote_footer('--- Rothman, Greenland, Lash (2017): Modern Epidemiology, 3rd edition')`

Thus, we need to create a variable which stores the time of the exposure = the time of the dinner. Recall how in the first part of the case study, we used the `lubridate` package to handle time-variables. You may find the [*lubridate cheat sheet*](https://raw.githubusercontent.com/rstudio/cheatsheets/main/lubridate.pdf) helpful. 

```{r}
cph <- cph %>% 
  mutate(
         # Create variable storing the time of the dinner (equal for all participants)    
         dinner_dayhour = dmy_h("11.11.06 - 18"),
         # in case you havenÂ´t done it above:
         onset_dayhour = onset_day + hours(onset_hour),
         # calculate incubation period
         incubation = onset_dayhour - dinner_dayhour
         )
```

Now we can look at the distribution of incubation times visually.

```{r}
ggplot(data=cph %>% filter(!is.na(incubation))) +
  geom_histogram(aes(x = incubation), binwidth = 6) +
  # adapt scale to better fit data
  scale_x_continuous(breaks = seq(0, 48, 6)) + 
  labs(x = "incubation period in 6-hour bins",
       y = "number of cases")
```


### Calculate attack proportions  

You can calculate crude attack proportions and attack proportions stratified by different variables of interest. Commonly, *attack proportions* are also called "*attack rates*" but please note that "*attack rate*" is a misnomer and should not be used: "*attack rates*" are not *rates*, but rather *proportions*. Mathematically, a *rate* is defined as a change in one unit per change in another unit. For example, think about velocity: the distance traveled per time passed, typically measured in *meters*/*seconds*. Other examples include decay of radioactive material with *decays*/*seconds*, or widely used in epidemiology: *incidence rate* - the number of new cases per month/year/etc. Crucially, *rates* have no upper limit: 10, 100, 1000 individuals can fall sick per month. *Proportions*, in contrast, compare the amount of something to the amount of the whole. *Attack proportions* compare the number of those having become sick to the size of the whole sample. As such, *attack proportions* can never be greater than 1 or 100% - at maximum, the entire sample became ill. 

Thus, the *attack proportion* is simply the percentage of Cases among the total observed individuals and can be calculated quite easily: 

```{r}
tabyl(linelist, case) %>% 
  adorn_totals(where = "row") %>% 
  adorn_pct_formatting(digits = 1)
```
The overall attack proportion is 215/377 = 57 % 

Does the attack proportion differ, when we compare teachers and students?

```{r}
tabyl(cph, group, case) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting(digits = 1) %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(col_name = "case") 
```

In the students, the attack proportion is 209/362 = 57.7%, in the teachers, the attack proportion is 6/15 = 40%. Of course, we would like to to know if attack proportions differ when stratifying by other variables, too. 

Use the `tbl_summary` function from the gtsummary package to calculate attack proprtions for group, class and sex by case status.

```{r}
cph %>% 
  select (group, class, sex, case) %>% 
  tbl_summary(by=case) %>% 
  add_overall() %>% 
  add_p()
```

Optional: Alternatively, for this task, we will be using `map()` again. This time, however, it is slightly more complex because for stratification, `tabyl()` requires two inputs: one for the variable we are investigating and one for `case`. Thus, we can not just feed it our data.frame/list `cph` as we did earlier. Instead, we are going to hard-code tabyl to use `cph` and `case` and feed it the variable name that changes for each iteration. LetÂ´s define the variables we are interested in: 

```{r}
# vars we want to stratify by
vars <- c("group","class", "sex")
class(vars)
```

As you can see, we created a vector of type `character` that stores the names of the variables we are interested in. We are going to map `tabyl()` onto this vector; conceptually, we will be repeatedly performing `tabyl()` with the variable name changing for each iteration. Thus the parameter that changes with each iteration is the *variable name*. When we refer to a *name* parameter within `map()` (and within all tidyverse functions!) we need to use the `.data[[NAME]]`- notation whenever the parameter is called. The reasons for this behaviour are quite complex and lie in the way R evaluates arguments - for our purpose it is really not necessary to understand. Just remember: ***when the parameter that changes is a name, refer to it with `.data[[]]`***.


```{r}
vars %>% 
  # give names to the output-list for a nicer display later on
  set_names() %>%
  # map tabyl() to each element of 'vars'
  map(.x = .,
      .f = ~ tabyl(cph, .data[[.x]], case) %>%
        # When map()-ing over names, 
        # use the .data[[.x]] argument when referring to the name
        adorn_totals(where = "col") %>% 
        adorn_percentages() %>% 
        adorn_pct_formatting(digits = 1) %>% 
        adorn_ns(position = "front") %>% 
        adorn_title(col_name = "case")
      )
```
\pagebreak


# 3. Univariable analysis 

To identify (the) potential vehicle(s) in the outbreak, proceed with an analytical study where you use statistical tests to investigate the associations of some suspicious food items with the disease.

## Section goals

In this section you will learn how to 
 - use the `epi.2by2`-function to estimate measures of association for categorical data
 - perform hypothesis tests for categorical data
 - investigate if dose-response relationship is present for categorical data

## Task 06: Conduct univariable analysis 

Carrying out a retrospective cohort study,

 - Calculate appropriate measures of association and 95% confidence intervals for food exposures
 - perform appropriate hypothesis tests for the variables you identified
 - Check for a dose-response relationship between the food items you identified and being a case
 - After each step, interpret your results 

## Help for task 06 {.tabset .tabset-pills}

### Calculate measures of association

As always, in `R`, there is more than one way to create univariable tables. Here, we will mainly be using the `epi.2b2()`-function from the `epiR`-package, because it automatically calculates Odds Ratios or Risk Ratios and computes other relevant parameters such as attributable risk or incidence rates. If you were interested in the pure table without any further information, use `table()` from base-R or `tabyl()` as we did earlier.

In the next section, you will be stratifying your analysis. Already think about which variables you might consider potential effect modifiers or confounders. One common strategy is to base this decision on the univariable results obtained and a p-value threshold of 0.25. What do you think about this approach? In our case, additionally, food items that are known risk factors for gastroenteritis could also be included regardless of their univariable p-value.

The `epi.2by2` function from the `epiR` package can be used to calculate both risk and odds ratios. You can find out more information on the function by calling the help-function `?epi.2by2`. **Pay particular attention to the format, `epi.2b2()` requires its input to be in!** - How can you transform your variables, so that they have the desired format? 

---

<div style="display: none;" id="hidden_text4"> 
When using `method = "cohort.count"` or `method = "case.control"` to calculate Risk Ratios or Odds Ratios, `epi.2by2()` requires as input a table with a very specific format: "Disease +", "Disease -" and the same applies to Exposure. Note that the variables we are using are `logical` variables that can either be `FALSE` or `TRUE`. Thus, if you build a table with those variables the table will be in the wrong order! "Disease -", "Disease +". To turn around the order, we use a small trick: When transforming the variables from type `logical` to type `factor` we can specify the order of levels. Try it out: 

```{r results="hide"}
factor(cph$case, levels = c("TRUE", "FALSE"))
```


</div>
<button href="#" onclick="$('#hidden_text4').show(); return false;">Click here to show spoiler 
</button>

You can now transform all the variables of interest to factor variables.

```{r }
cph <- cph %>% 
  mutate(across(
      .cols = c("case", "pasta", "veal", "champagne", "sauce", "shrimps"),
      .fns = ~factor(., levels = c("TRUE", "FALSE")))
  )
```

---

As a first example, letÂ´s have a look at the risk for being a case by exposure to shrimp in a cohort study design. The epi.2by2 function requires data to be in a table format with a very specific order of factors (see the help-function). 

```{r}
# Create a cross-table with exposure and outcome variables
cph %>% 
  select(shrimps, case) %>% 
  # recode variables, so that they have the order, 'epi.2by2' expects
  mutate(case=factor(cph$case, levels = c("TRUE", "FALSE"))) %>% 
  mutate(shrimps=factor(cph$shrimps, levels = c("TRUE", "FALSE"))) %>% 
  table() %>% 
  # apply epi.2by2 function to the table
  epiR::epi.2by2(method = "cohort.count")
```
Note, that the totals differ slightly from the case/non-cases - thatÂ´s because for some of the individuals, `shrimps` is NA - the information is unknown. 

You can also estimate risk ratios for several exposures using the `cstable` from the EpiStats package.

```{r}
#Shrips as exposure
Table_shrimps <- EpiStats::cstable(as.data.frame(cph), 
                             cases = "case", 
                             exposure = c("shrimps"))

Table_shrimps

```
```{r}
#Table for mutliple exposures
Table_all <- EpiStats::cstable(as.data.frame(cph), 
                             cases = "case", 
                             exposure = c("shrimps", "pasta", "veal", "champagne", "sauce"))

Table_all

```


Optional: To make things faster, we once again `map()` the `epiR::epi.2b2()`-function and the steps we did above to all the variables we are interested in; 

```{r echo = TRUE, output.lines = 12}
# which exposures are we interested in?
exposures <- c("pasta", "veal", "champagne", "sauce", "shrimps")

exposures %>% 
  set_names() %>%
  # map the following to each element of 'exposures'
  map(~ cph %>% 
        select(.data[[.x]], case) %>% 
        mutate(across(.cols = everything(), 
                      # recode variables, so that they have the order, 'epi.2by2' expects
                      .fns = ~ factor(., levels = c("TRUE", "FALSE")))) %>% 
        table %>% 
        epiR::epi.2by2(method = "cohort.count")
      )

```


### Perform hypothesis tests

**01. Is gender associated with being a case?** 

For Hypothesis-testing, we are going to use the `chisq.test()`, `wilcox.test()`, and `fisher.test()`-functions. For now, call `?chisq.test()` to find out more about the function. Which inputs does it require? What are the standards for each of the function's arguments? Do they seem reasonable for our application? 

---

<div style="display: none;" id="hidden_text2"> 
Note that `chisq.test()` expects as an input either two vectors of type `numeric` or a `matrix`. A `matrix` is, for example, what we get by using `table()`. You can always test if your OBJECT is a matrix by calling `is.matrix(OBJECT)`. For example, try calling `is.matrix(table(cph$case, cph$sex))`. Using `table()` over passing to vectors of type `numeric` for `chisq.test()` has the advantage that `table()` handles all kinds of vectors easily - you do not have to convert them into type `numeric` first.
</div>
<button href="#" onclick="$('#hidden_text2').show(); return false;">Click here to show spoiler 
</button>

---

```{r}
# using a chi-squared test
chisq.test(table(cph$sex, cph$case))

# alternatively, tidyR version:
cph %>% 
  # note the use of 'taybl()' instead of 'table()'
  tabyl(sex, case) %>% 
  chisq.test()
```

**02. Is class associated with being a case?**

Here you can either compare proportions, using the chi-squared test as above, or use the Wilcoxon-Mann-Whitney test to compare distributions. Call `?wilcox.test()` to learn more about the function. Which inputs does it require? What are the standards for each of the function's arguments? Do they seem reasonable for our application?


```{r}
# wilcox.test() expects an input of two vectors of type 'numeric'
# which type are 'class' and 'case'?
class (cph$class)
class (cph$case)

# change 'case' from a logical to numeric
cph <- cph %>% 
  mutate(across(.cols =  c(class, case),
                .fns =  ~as.numeric(.),
                # specify names of the new columns 
                # {.col} will be replaced by the actual name
                # _num stands for 'numeric'
                .names = ("{.col}_num")
                )
         )

# run the wilcoxon 
wilcox.test(cph$class_num, cph$case_num)

```

**03. Is there a difference between age in males and females?** 

Remember, `wilcox.test()` expects two vectors of type `numeric`. Here, `sex` is type `factor`. How would you turn factor-type information into numbers? 

```{r}
# what does 'sex' look like?
glimpse(cph$sex)
```

As you can see, R represents `factor`-variables as numeric variables. `sex` is a vector of `1`s and `2`s. What sets apart `factor` variables from ordinary `numeric` variables is that every value is connected with a label and that only values can be represented, that have been assigned with a label. Extract `sex` to an object and try adding a value that has not been defined such as "diverse" - what happens?

<div style="display: none;" id="hidden_text3"> 

```{r}
tmp_sex <- cph$sex

tmp_sex[12] <- "diverse"
tmp_sex[12] <- 3
```

</div>
<button href="#" onclick="$('#hidden_text3').show(); return false;">Click here to show spoiler 
</button>

To work with factors, the `forcats`-package is really helpful and makes your factor-life easy (See: [*forcats* webpage](https://forcats.tidyverse.org/)). For now, we can use the fact that R represents factors as numerical under the hood to our advantage: We tell R to treat `sex` as numeric and thus discard the additional label-information:

```{r}
cph <- cph %>% 
  mutate(sex_num=as.numeric(cph$sex))
```


```{r}
wilcox.test(cph$age, cph$sex_num)
```

Alternatively, if `age` is approximately normally distributed, we could also perform a *t-test*: Call the help-function to learn more about `t.test()`. First, however, we should estimate, if `age` indeed approximates the normal: One way to do this is by using the *shapiro*-test - `shapiro.test()`. 

```{r}
# shapiro tests if significantly different from normal distribution
shapiro.test(cph$age)
```
```{r}
# now perform the t.test
# note the formula-notation that uses the '~'-symbol
t.test(cph$age ~ cph$sex)
```


### Estimate dose-response-relationship

Look at the food items that seem most suspicious to you. Hint: it's pasta and veal!

There is two possible ways to do this. The first is to use the `wilcox.test()` function again, and the second is to calculate odds ratios using the `epi.2by2()` function. Using the `epi.2by2()` function for dose-responses gives more detailed results (traditionally this would be done using regression), so we have not shown it here; if you would like to see the code for this it is available in the appendix. 

```{r}
# Remember: wilcox.test() expects an input of two vectors of type 'numeric'
is.numeric(cph$vealD)
# lucky us!

# run the wilcoxon 
wilcox.test(cph$vealD, cph$case_num)
```

Optional: We can speed up things by using `map()` to analyze the dose-Response of multiple variables at the same time. 

```{r,output.lines = 12}
vars <- c("vealD", "pastaD", "champagneD", "shrimpsD")

vars %>% 
  set_names() %>%
  map(.f = ~ wilcox.test(data = cph, 
                         as.formula(str_c(.x,"~","case_num"))))
```



\pagebreak

# 4. Stratified analysis 

In the last section we saw that those eating pasta or veal as well as drinking champagne have the highest risk of becoming ill. There are, however, many other food items that are associated with an increased risk. Consequently, we need to check for effect modification and confounding by stratifying the analysis. 

## Section goals
In this section of the case study you will learn to

  - perform simple stratified analysis using the *Mantel-Haenszel*-approach
  
Please note that in reality, you would much more likely be using *Regression Models* to account for confounding and check for effect modification. The use of the *Mantel-Haenszel*-approach explained here is an intermediate step chosen for didactical purpose and leads to introduction of regression modelling in the next *EPIET* modules. 

## Task 07: Performing stratified analysis

  - Stratify your analysis by having eaten veal, having eaten pasta, and having drunk champagne
  - describe your findings: Do you see any confounding? Did you find effect modification?

## Help for task 07 

```{r}
# Make a 3-way table with exposure of interest, the outcome and the stratifying variable, in that order
mh <- cph %>%
  select(veal, case, pasta) %>% 
  # recode variables, so that they have the order, 'epi.2by2' expects
  mutate(across(.cols = everything(), 
                .fns = ~ factor(., levels = c("TRUE", "FALSE")))) %>%
  table() %>% 
  epi.2by2(method = "cohort.count")

mh
```

`epi.2by2` produces much more output than what is shown initially: You can check this by using `str()` (for *structure*) on the `epi.2by2`-object we created and assigned as `mh`. Under the hood, `mh` is a `list` object - a mixture of different dataframes, vectors, and even further lists. Recall, how we encountered `list`-objects before, when using `map()` -  `list` objects are extremely versatile and become indispensable, once you start working with more complex analysis and building models. For a detailed introduction to `lists` please refer to [*Advanced R*](https://adv-r.hadley.nz/vectors-chap.html#lists) or [*The Epidemiologist R Handbook* section on *iteration](https://epirhandbook.com/en/iteration-loops-and-lists.html#iter_purrr).

For now, it may suffice to note that to index elements within lists, you may need to use several levels of indexing. Recall how we indexed our dataset using the `$`-operator, for example: `cph$age`. In lists, two levels of `$` signs may be needed, such as `mh$massoc.detail$RR.strata.wald`: First, you index the list in its first level, by choosing `$massoc.detail`, then you index `massoc-detail` by choosing `$RR.strata-wald`. Let`s try:

```{r}
#Explore mh
mh
names(mh)
names(mh$massoc.detail)

# Crude risk ratio:
mh$massoc.detail$RR.crude.wald 

# Stratum-specific risk ratios:
mh$massoc.detail$RR.strata.wald

# Adjusted risk ratio:
mh$massoc.detail$RR.mh.wald

#Difference percentage between crude and adjusted risk ratio
diff_per <-round(((mh$massoc.detail$RR.crude.wald-mh$massoc.detail$RR.mh.wald)/mh$massoc.detail$RR.crude.wald)*100, digits=0)

```

If you want to, you can patch together your very own output:

```{r}
results <- bind_rows(mh$massoc.detail$RR.crude.wald, 
                     mh$massoc.detail$RR.strata.wald, 
                     mh$massoc.detail$RR.mh.wald,
                     diff_per)

# We can label the rows of this table as below
rownames(results) <- c("Crude", "Stratum 1", "Stratum 0", "Adjusted", "Difference")

# view output table:
results
```

Now do the same for other exposures.

Optional: To speed up the process, we are, once again, going to use `purrr::map()` to apply the analysis to several variables of inetrest at once:

```{r, output.lines = 12}
# list the variables you are interested in
vars <- c("veal", "rocket", "shrimps", "champagne", "sauce")

# map the function to the variables
vars %>% 
  set_names() %>% 
  map( ~ cph %>%
         select(.data[[.x]], case, pasta) %>% 
         # recode variables, so that they have the order, 'epi.2by2' expects
         mutate(across(.cols = everything(), 
                       .fns = ~ factor(., levels = c("TRUE", "FALSE")))) %>%
         table() %>% 
         epi.2by2(method = "cohort.count")
  )
```

The exact same can be done for veal (switch veal and pasta):

```{r, output.lines = 12}
# list the variables you are interested in
vars <- c("pasta", "rocket", "shrimps", "champagne", "sauce")

# map the function to the variables
vars %>% 
  set_names() %>% 
  map( ~ cph %>%
         select(.data[[.x]], case, veal) %>% 
         # recode variables, so that they have the order, 'epi.2by2' expects
         mutate(across(.cols = everything(), 
                       .fns = ~ factor(., levels = c("TRUE", "FALSE")))) %>%
         table() %>% 
         epi.2by2(method = "cohort.count")
  )
```

It appears that pasta confounds the association between eating veal and being a case. For a variable to be a confounder it needs to be associated both with the outcome (being a case) and with the exposure. We know from univariable analysis that pasta is associated with being a case; we can now check if it is also associated with veal.

```{r}
# Using a fisher's exact test:
fisher.test(table(cph$pasta, cph$veal))
```



# 5. Conclusions of the case study 

## Descriptive analyses {-}

We didn't find anything surprising in the descriptive analysis of the cohort's age given it consists of two groups, the students and the teachers.

The distribution of the cohort regarding sex, group and class also didn't reveal anything unusual. Students seem a bit more affected by the outbreak than teachers and the attack rate is higher for older students in higher classes. This, however, is a purely descriptive result.

When constructing an epicurve, we need to decide on the resolution, i.e. the time interval for a single bar. A rule of thumb is to use one third or one fourth of the average incubation period as an interval. For our investigation this means we should use approximately a 6h interval.

This seems a good choice indeed as we saw that the daily interval was too coarse to really see the signal we're after. The epicurve and the summary of the incubation period show that there seemed to be a rapid onset of symptoms following exposure. This is in line with our previous suspicion that a virus or a toxin might be the causative agent in the outbreak.

The unimodal shape with the sharp peak suggests a point source, while the tail on the right hand side could be explained by secondary cases or background noise. Also people that only consumed a little contaminated food and therefore only a low infectious dose could have a longer incubation period and could explain the late cases.
The above results are in line with norovirus as the prime suspect, but the symptoms are not a textbook fit. There are too few people that experienced vomiting!


## Univariate analyses {-}
The interesting results here are that the two food items that are most suspicious are pasta and veal. In particular, the dose response relationship that was found for pasta points towards pasta as the potential vehicle. Pasta as such is unlikely to be contaminated, but as you can see in the label, it was served with pesto!
Before you jump to conclusions, be aware that this result could be due to confounding! Maybe pasta was clean but eaten by all the people who ate the food item that actually was contaminated!

## Stratified analyses {-}
We found that pasta consumption confounds the association between eating veal and being ill. The crude (univariable) result for veal suggests that veal is a risk factor (crude RR = 1.52, CI: 1.00-2.31), but when we adjust for the consumption of pasta we see that actually this result is due to the fact that most people who ate veal also ate pasta.
Within the stratum of the people who ate pasta, veal has no effect (RR = 1.19, CI: 0.59-2.40). The same holds within the stratum of people who didn't eat pasta (RR = 1.05, CI = 0.38, 2.92). This is why the adjusted MH-RR also suggests that veal has no effect (RRadj = 1.14, CI: 0.64-2.03).
This result taken together with the dose response relationship we found earlier for pasta gives additional evidence that there was something going on with the pesto!

## Conclusions of the investigation {-}
In summary, it is fair to say that there was both epidemiological and microbiological evidence that the pasta salad with pesto was the most likely vehicle of transmission in this outbreak. Further investigations focused on how the pasta salad with pesto could have become contaminated and on lessons learned from this outbreak that could then communicated both the scientific community, the caterer and the general public.


## Task 08: Summary and reflecting the case study 

Summarize your analysis and findings: 

  - Recall the steps you undertook in this case study and describe the intermediate findings at each step
  - Which difficulties did you have at each of the steps? Could any of these have been prevented by setting up data collection differently or changing anything prior to data analysis?
  - Which new `R` concepts did you learn?
  
## Bonus-task 08b: Analysis Report using *R markdown*

If you want to, create an analysis report that describes the steps you undertook and summarizes your findings utilizing `markdown` and `knitr`. Include visualizations, such as tables, graphs, and the epicurve. While voluntary, you will be profiting greatly from this exercise because this task involves going through your documents and scripts again, helping with recall and repetition. Reports like this will be required from you in each and every outbreak investigation. Using `R` for creating reports rather than other software like *MS word* the advantage of making your research *reproducible*:

  - Your report is the result of *code* - as such it can be executed by anyone and will always yield the same result
  - Everything is in one place: no juggling of different files, manual changes, different versions
  - The document will serve as a record for how you arrived at the results you include in your papers - this makes your research transparent and is immensely helpful to defend your findings against criticism
  - Dynamic documents can change as data are updated

For more information and a practical introduction to `markdown`, please refer to [*R Studio* Introduction to *R markdown*](https://rmarkdown.rstudio.com/lesson-1.html),  [*R for Data Science* chapter on *reproducible research*](https://r4ds.had.co.nz/r-markdown.html), and the [*R markdown* cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf).

# References

  Batra, N., Mousset, M., Spina, A., Florence, I., Liza, Aminatand, Laurenson-Schafer, H., Fischer, N., Molling, D., Polonsky, J., Bailey-C, Ebuajitti, Blomquist, P., Campbell, F., Hollis, S., Whoinfluenza, Llhaskins, Yurie, & Michellesloan. (2021). appliedepi/epiRhandbook_eng: v1.0_eng initial release (v1.0_eng) [Computer software]. Zenodo. https://doi.org/10.5281/ZENODO.4752646
  
  Morgenstern, H., Kleinbaum, D. G., & Kupper, L. L. (1980). Measures of Disease Incidence Used in Epidemiologic Research. International Journal of Epidemiology, 9(1), 97â€“104. https://doi.org/10.1093/ije/9.1.97
  
  Rothman, K. J., Lash, T. L., VanderWeele, T. J., & Haneuse, S. (2021). Modern epidemiology (Fourth edition). Wolters Kluwer.
  
  Wickham, H. (n.d.). Tidy Verse - R packages for data science. https://www.tidyverse.org/
  
  Wickham, H. (2009). Ggplot2: elegant graphics for data analysis. Springer. https://ggplot2-book.org/
  
  Wickham, H. (2019). Advanced R (Second edition). CRC Press/Taylor and Francis Group. https://adv-r.hadley.nz/
  
  Wickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data (First edition). Oâ€™Reilly. https://r4ds.had.co.nz/




