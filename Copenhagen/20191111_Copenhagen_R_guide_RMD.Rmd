---
title: "Outbreak of gastroenteritis after a high school dinner in Copenhagen, Denmark, November 2006"
author: "Amy Mikhail (adapted for the UK FETP from previous EPIET versions)"
date: "11 - 15 November 2019"
output:
  html_document: 
      toc: yes
      toc_float:
        collapsed: no
        smooth_scroll: yes
  pdf_document: 
      toc: true
      toc_depth: 3
urlcolor: blue
---

```{r restrict output, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}

################################################################
# FUNCTION TO RESTRICT RESULTS TO A FEW LINES OF OUTPUT:

# Check if knitr is installed, if not install it:
if (!requireNamespace("knitr", quietly = TRUE)) install.packages("knitr")

# Load knitr
library(knitr)

# Define empty output:
hook_output <- knit_hooks$get("output")

# Function to restrict lines of output:
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

``` 


# Copyright and license
This case study was designed under a European Centre for Disease Prevention and Control (ECDC) service contract for the development of training material (2010). The data were slightly modified for training purposes.

The following code has been adapted from the STATA case study companion guide to **R** for learning purposes. The initial contributors and copyright license are listed below. All copyrights and licenses of the original document apply here as well. 

**Previous authors and ontributors to this R companion guide:** 

 - Alexander Spina
 - Patrick Keating 
 - Daniel Gardiner(PHE) 
 - Lukas Richter (AGES)
 - Ashley Sharp (PHE)
 - Hikaru Bolt (PHE)

**Source :**
This case study is based on an investigation conducted by Jurgita Pakalniskiene, Gerhard Falkenhorst (Statens Serum Institut, Copenhagen) and colleagues.

**Case study authors:**
Jurgita Pakalniskiene, Gerhard Falkenhorst, Esther Kissling, Gilles Desv

**Reviewers:**
Marta Valenciano, Alain Moren.

**Adaptions for previous modules:**

 - Irina Czogiel
 - Kristin Tolksdorf
 - Michaela Diercke
 - Sybille Somogyi
 - Christian Winter
 - Sandra Dudareva-Vizule
 - Katharina Alpers
 - Alicia Barrasa
 - Androulla Efstratiou
 - Steen Ethelberg
 - Annette Hei?enhuber
 - Aftab Jasir
 - Ioannis Karagiannis
 - Pawel Stefanoff


\pagebreak 


**You are free:**

- to Share: copy, distribute and transmit the work
- to Remix: adapt the work, under the following conditions:

**Attribution:**

- You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). 
- The best way to do this is to keep as it is the list of contributors (sources, authors and reviewers).

**Share Alike:**

- If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. 
- Your changes must be documented. 
- Under that condition, you are allowed to add your name to the list of contributors.

**Use in teaching and training:**
You cannot sell this work alone but you can use it as part of a teaching programme, with the understanding that:

- Waiver: Any of the above conditions can be waived if you get permission from the copyright holder.
- Public Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.
- Other Rights - In no way are any of the following rights affected by the license:
    + Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations;
    + The author's moral rights;
    + Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.
- Notice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.

This licence is based on [http://creativecommons.org/licenses/by-sa/3.0/](http://creativecommons.org/licenses/by-sa/3.0/)

\pagebreak 


# Objectives:

At the end of the case study, participants should be able to:

- Conduct an investigation to identify the source of an outbreak and apply the ten steps of an outbreak investigation
- Explain the contributions of epidemiological and microbiological investigations used in foodborne outbreaks
- Carrying out data cleaning using R
- Performing descriptive, univariable and stratified analyses using R
- Critically evaluate the results from statistical and microbiological analyses and identify food vehicles most likely associated with becoming ill
- Understand the difficulties in food trace-back in today's globalised world (in plenary)
- Understand the importance of writing outbreak reports (developing an analytical plan)

\pagebreak 

# 01. The Alert 

On November 14th 2006 the director of a high school in Greater Copenhagen, Denmark, contacted the regional public health authorities to inform them about an outbreak of diarrhoea and vomiting among participants from a school dinner party held on the 11th of November 2006. Almost all students and teachers of the school (750 people) attended the party. 

The first people fell ill the same night and by 14 November, the school had received reports of diarrhoeal illness from around 200 - 300 students and teachers, many of whom also reported vomiting.


# Getting started

Your group has been tasked with investigating this outbreak; you have just received the information above.  Before you spring into action, sit together and make a plan. Think about the ten steps of an outbreak investigation and how they apply in this setting. What practical issues might occur?

The particular focus of this session should be on steps 1-4, i.e. the steps you need to take before you sit down and analyse the data.

## Task 01. Planning the investigation and first steps:

These are some things you may want to think about:

- Do you think this is a real outbreak?
- Based on the available information (e.g. clinical symptoms, incubation period), what kind of pathogen do you suspect at this stage?
- What further investigation would you conduct to confirm the diagnosis?
- What kind of case definition would you use for case finding? 
- How would you carry out the case finding in this setup? 
- Also think about an effective way of obtaining information about non-cases?
- Would you carry out an analytical study in this setting? 
- In case you decide to do a cohort study, how would you define the cohort?
- What can you do to get a good response in your study?
- What kind of additional investigations would you carry out?

\pagebreak 

# 02. Data management and different file types

Investigators used a questionnaire similar to the one provided to you, but more exposures were included. Data were captured directly by the online survey tool. These data were then exported from the survey tool and saved as `Copenhagen_raw.csv`.

In this session you will learn:

 - how to navigate the RStudio environment
 - how to install and load R packages (groups of functions)
 - how to import and organise your data
 - how to save commands and the results they produce in an R markdown script


## Task 02. Becoming familiar with RStudio and importing the data

 - Download `Copenhagen_raw.csv` from the Sharepoint module folder
 - Save this file in a clean folder on your computer
 - Open RStudio
 - Create a new `.Rproj` file in the same folder as your data set
 - Create a new R notebook and save this in the same folder
 - Add the title of this case study and today's date to the top of the R notebook
 - Set this folder as your working directory

\pagebreak  

## Help for task 02:

**An introduction to the R companion**

>"To understand computations in R, two slogans are helpful:
>
>- Everything that exists is an object.
>- Everything that happens is a function call."
>
>`r tufte::quote_footer('--- John Chambers')`

If you look at the Global Environment panel (by default in the upper right of the screen in RStudio) you will see a list of objects stored in that environment. When you import your data into R you create an object. This is completely separate from the data file itself (the excel file, or csv file etc.). You can create as many objects as you like, for example you could store a few variables from your original data as a new object, or create a summary table and store that. 

Functions in R are equivalent to commands in STATA. All functions take the form of a name followed by brackets e.g. `functionname()`. Inside the brackets go various arguments. You can access the help file for a function by calling `?functionname`. The help file will show which arguments the function takes and what the function does. Arguments have a default order, as specified in the help file, though you can override this by specifying which argument you are entering using the equals sign `"="`.

A good reference for R users is the book R for Data Science by Garrett Grolemund and Hadley Wickham. This is available free online at [http://r4ds.had.co.nz/](http://r4ds.had.co.nz/).

### RStudio projects

The easiest way to work with R is using RStudio 'projects'. RStudio is a graphical user interface that runs R in the background. A 'project' is an RStudio file that saves your workspace so you can easily pick up from where you left off. Put all the files that you will need for this case study in a folder called 'Copenhagen' and create a project in the same folder by clicking `file -> new project -> existing directory`, and choosing the folder. For simplicity, make sure there are no subfolders in this folder, and put all data and scripts in the main Copenhagen folder. 


### Installing packages and functions

R packages are bundles of functions which extend the capability of R. Thousands of add-on packages are available in an online repository called the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org) and many more packages in development can be found on GitHub. They may be installed and updated over the Internet.

R and Stata have minor differences in default settings and methods. In this document we will follow the Stata analysis as closely as possible, but small and usually unimportant differences may be noted between the statistical findings in R and those in Stata. At some points additional steps (which would usually be optional in R) will be taken to produce output which is comparable to that of Stata.

\pagebreak

We will mainly use packages which come ready installed with R (base code), but where it makes things easier we will use the following add-on packages from CRAN:

 - `devtools`
 - `here`
 - `haven`
 - `data.table`
 - `lubridate`
 - `ggplot2`
 - `Hmisc`
 - `epiR`
 - `epitools`
 - `openxlsx`

In this case study, we are also going to use a package developed by UK FETP alum Daniel Gardiner, called `EpiFunc`. Daniel has stored this package on Github; you can view some information about the package including instructions on how to install it and example use [here](https://github.com/DanielGardiner/EpiFunc). 

The first thing you will need to do is install all the packages you need - to do this, run the following code just once at the beggining of this module (note: requires an **internet connection** as the packages will be downloaded from online repositories):

```{r, echo = TRUE, results='hide'}

# List required CRAN packages to install:
pkglist <- c("devtools",
             "tidyverse",
             "here",
             "lubridate",
             "Hmisc", 
             "epiR", 
             "epitools",
             "skimr")
             # "EpiFunc"  --  uncompatible with R 4.x.x

# Check if packages are already installed, if not install them:
if (!requireNamespace(c(pkglist), quietly = TRUE)) install.packages(c(pkglist))

# Check if EpiFunc (Github package) is installed, if not install it:
if (!requireNamespace("EpiFunc", quietly = TRUE)) devtools::install_github("DanielGardiner/EpiFunc")

```


### Setting your working directory 

Just as in STATA you can set a folder to be your working directory (using the `setwd()` command). Open the project that you've created and you will see that the working directory defaults to the same folder as the `.Rproj` file: you can check this by calling `getwd()`. You can see what is in your working directory by looking at the **Files tab** (by default in the bottom right panel of RStudio). If you want to set your working directory you can use the function `setwd()`. Note that R file paths use forward slashes "/", while windows file paths use back slashes "\\";  if you copy a path from windows you have to change them manually (using the find and replace button at the top left of the RStudio panel)

Another option for setting working directories is to use the `here` package; this automatically detects the directory where your `.Rproj` file is stored.  You can also use the function `set_here()` from this package to set a place-holder in the current directory and use that to set the working directory.  This is very useful when sharing your code, as you can set the working directory at the beggining of your script without explicitly referring to a file path or network drive that others may not have access to.  

```{r set working directory, echo=TRUE, message=FALSE, results='hide'}

# Check the location of your current working directory:
getwd()

# If happy with this location, set a placeholder in the current directory:
here::set_here()

# Set the working directory relative to the location of the .here file you just created:
setwd(here::here())

```


Lastly, we will also install a function called `sva` analagous to STATA's `cctable` and `cstable` commands, to perform single variable analysis and calculate attack rates, risk ratios or odds ratios over multiple exposures.  This function has been provided to you as an R script which you should store in your working directory.  You will need to source (i.e. make available in R) this function at the beggining of each new R session.  This is a user-written function created by Daniel Gardiner, analogous to the user-written ado files in STATA.  Unlike STATA, in R you can store user-written functions wherever you like; when importing (sourcing) them you would simply provide the full file path to that function's location on your computer. In this example, Daniel has also stored the function in a Github repository, so we will source it using the URL to the `raw` version of this file.  `Raw` files on Github or Gitlab are simply text files containing code.  What you see when you first click on a link to a file in a Git repository is the HTML formatted version of the code, which R is not able to read.  There is usually a button at the top right of the Git repository web page that allows you to view (and copy the URL of) the `raw` version.


Run the following code at the beginning of each day to make sure that you have made available all the packages and functions that you need. Be sure to include it at the top of any scripts too.

The full URL to the `sva` function is here:


[https://raw.githubusercontent.com/DanielGardiner/UsefulFunctions/master/single.variable.analysis.v0.3.R](https://raw.githubusercontent.com/DanielGardiner/UsefulFunctions/master/single.variable.analysis.v0.3.R)


```{r, echo = TRUE, results='hide', message=FALSE, warning=FALSE}

# Make a list of the packages you want to load:
pkgs2load <- c("devtools",
               "tidyverse",
               "lubridate",
               "Hmisc", 
               "epiR", 
               "epitools",
               "skimr"
               )

# Loop through the packages in the list with lapply and load them:
lapply(c(pkgs2load), library, character.only = TRUE)

# Source the single variable analysis function:
source("https://raw.githubusercontent.com/DanielGardiner/UsefulFunctions/master/single.variable.analysis.v0.3.R")

```


### Reading in datasets:

Open the data set `copenhagen_raw.csv` using the `read_csv()` command.  
This dataset has been exported from the survey software and saved as a `.csv` file.  
It is also possible to import datasets from other formats, such as excel or stata; see appendix for example. 
Datasets in `R` are stored and can be referred to using the name that has been assigned to it (in our case **"cph"**).
R can hold one or many data sets in memory simultaneously, so there is usually no need to save intermediate files or close and re-open datasets.


```{r, echo = TRUE, results='hide', message=FALSE, warning=FALSE}

# Read in your data from a csv file 
# use read_csv() as opposed to read.csv()
cph_raw <- read_csv("Copenhagen_raw.csv")  

# create a copy
cph <- cph_raw
```

### Browsing your dataset 

`Rstudio` has the nice feature that everything is in one browser window, so you can browse your dataset and your code without having to switch between browser windows. 

```{r, echo = TRUE, eval=FALSE}

#To browse your data, use the View command
View(cph)

```

```{r, echo = TRUE, warning=FALSE}

# To see the dimensions (number of rows and columns) of your dataset, use dim()
dim(cph)

```

Alternatively, you can also view your dataset by clicking on the `cph` object in the top right "global environment" panel of your `RStudio` browser.  
Your global environment is where you can see all the datasets, functions and other objects you have loaded in the current session.


### Saving your code in R Scripts

R scripts are the equivalent of .do files in STATA. You can save your code in these R scripts. You can create a new script by clicking the top left icon (below 'File') and choosing "R Script", or entering "Ctrl+shift+N".

You can write comments in your code using "#"

Log-files do not exist in R, however there is the History tab, in the global environment panel in the top right of your browser.  If you click on the code in the History tab, it will re-run.  

Alternatively, you could create an R markdown notebook, which will integrate text you write with calculations, figures, graphs and other outputs.  This format is very useful as R code can be split into 'chunks' that can be run separately and you can preview the output as you go.  R markdown notebooks are slightly simplified versions of R markdown documents, which you can use to create automated reports.  To create an R markdown notebook, go to `File --> New file --> R Notebook...` and follow the instructions to set up your document.


### Describing your dataset 

You can view the structure of your data set using the following commands. Each of these commands can be run for individual variables also. 

`dplyr::glimpse()` provides an overview of the structure of your data (number of observations and variable types):

```{r, echo = TRUE, warning=FALSE, output.lines = 9}
# View structure:
glimpse(cph)
```

`skimr::skim()` provides information on variable classes, completeness and number of missing values, as well as mean, sd, quartiles, and a rudimentary histogram for numeric variables:

```{r, echo = TRUE, warning=FALSE, output.lines = 20}
# Summarise data:
skim(cph)
```


# 03. Data cleaning and recoding in `R`

Now that we know our way around R and RStudio, we can begin checking the dataset for implausible values and other errors. Data checking and cleaning are important steps in any analysis. People say that it can take up to 80% of the analysis time when it is done properly.


## Task 03. Error checking and recoding

### Sub-task 03.01 Error checking

Check for errors using appropriate R commands - there are more than four different kinds of errors hidden in the `copenhagen_raw.csv` data!  Errors you may wish to look for include:

 - Incorrect assignment of variable types (for example dates stored as character strings)
 - Incorrect assignment of missing values (these need to be encoded as `NA` in R)
 - Incorrectly entered data (for example age entered as 150 is highly unlikely to be true)

Record and save the commands you use (and some annotation explaining what they are) in your R script or R Notebook, so that you can easily run them again.


### Sub-task 03.02 Recoding

You may have noticed the following errors - use appropriate R commands to correct (recode) these errors in your data set:

 - In the variable `age`;
     + Change the value 180 --> 18
     + Change the value 8 --> 18
     + Change the value 16 --> 61
 - In the variable `onset`;
     + Change the onset date to `NA` for respondents that didn't report symptoms


### Sub-task 03.03 Create case definition

Create a suitable case definition (logical variable in which cases = `TRUE` and non-cases = `FALSE` according to the criteria below):

In the study, a **case** was defined as:
 - a person from the cohort
 - presenting with diarrhoea, bloody diarrhoea, or vomiting
 - within 48 hours of their meal

Anyone who presented with diarrhoea or vomiting from 6pm on November 11th to 5:59pm on November 13th was included as a case.  All other participants including anyone with symptoms outside this time window are defined as "not ill" (as they probably didn't become sick at the party).

You may now wish to review your data and check if there are any respondents who fall outside the cohort of interest; if so, create a new subset of your data excluding these respondents and use this subset for further analysis.


\pagebreak

## Help for Task 03:

### Sub-task 03.01 Error checking:

You can examine a variable within a dataset using the `$` sign and then the variable name (e.g. `cph$age`). 

Alternatively, you can also refer to or subset a dataset using square brackets: 
 - the part before the comma refers to the rows 
 - the part after the comma refers to columns 
 
Columns and rows can be referred to by name (as a character string) or by their numeric index: 
 - By index: `cph[1:10,1:5]` first 10 rows and first 5 columns of the `cph` data
 - By name: `cph[,"age"]` gives you the variable age as a vector. 

You can subset a dataset using `[...]` in combination with logical statements using mathematical operators such as:

 - 'is equal to' (note the double equals sign) `==`
 - 'does not equal' `!=`
 - 'less than' `<`
 - 'greater than' `>`
 - addition, subtraction, multiplication and division: `+`, `-`, `*`, `/` 
 - 'and' (to bind multiple statements together - this is an ampersand) `&`
 - 'or' (to indicate that either of two statements must be met - 'bar' symbol not capital I) `|`
 - the order in which statements should be evaluated is indicated by enclosing them in parentheses `()`
 - to refer to missing values, use the `is.na(...)` logical function
 - to negate a statement, put an exclamation mark in front of it, e.g. `!is.na(...)` = 'not missing'
 - use the assign symbol to assign a name to a new object, data set or variable: `<-`

These are known as conditional statemets, which can be used to filter your data.  Conditional statements can be applied to any data type (numeric, text or dates).
    
New variables are easily created in R simply by using the `$` sign after the dataset name and writing a name not already in the dataset, then defining what should go in that variable, e.g.:

`cph$newvar <- cph$oldvar1 + cph$oldvar2`

... would create a new variable called 'newvar' which would be the sum of oldvar1 and oldvar2.

The `table(...)` function will create a frequency table.


```{r}
# Class() tells you what kind of object it is

class(cph)
# cph is a data frame, which is really just a table made up of a list of vectors

class(cph$age)
# Note that age is already a numeric variable which is correct
```

```{r}
# The table command will give a very basic frequency table (counts) 
# In this example the first line of the output is the age and the second is the frequency.
table(cph$age)
```

```{r}
# skimr::skim(VARIABLE) gives you detailed statistics on VARIABLE
skim(cph$age)
```



```{r}
# You can look at age among teachers using the group variable
table(cph$age[cph$group == 0])
# One teacher is too young! you wouldn't have noticed that without looking at the teacher separately!
```

```{r}
# You can look at age among pupils using the group variable
table(cph$age[cph$group == 1])
```

```{r}
# You can look at time people started becoming ill 
table(cph$starthour, exclude = NULL)
```


```{r}
# People did not have dinner but ate tuna, bread or veal
# You can label the table with deparse.level
table(cph$meal, cph$tuna, deparse.level = 2)
table(cph$meal, cph$bread, deparse.level = 2)
table(cph$meal, cph$veal, deparse.level = 2)
```

```{r}
# People with day of onset but no symptoms
# is.na() returns True/False if value is missing
# So this selects the participants that are either 0 or missing for all three symptoms
table(cph$dayonset[(cph$diarrhoea != 1 | is.na(cph$diarrhoea)) & 
                  (cph$vomiting != 1 | is.na(cph$vomiting) & 
                     (cph$bloody != 1 | is.na(cph$bloody))) ])

# instead of having to type out the condition for each variable, dplyr offers the `if_all()`-function:
# we ask if the condition holds true for all provided columns
# `if_any()` works analogously

cph %>% 
  filter(if_all(.cols = c(diarrhoea, bloody, vomiting),
                .fns = ~ (. != 1 | is.na(.))
                # .fns = ~ (. %in% c(0, NA)) better, but didactically worse?
                )) %>% 
  select(dayonset) %>% 
  table()
```

\pagebreak

### Sub-task 03.02 Recoding:

You can use combinations of the 'assign' symbol and conditional statements to recode data.

In addition, the `ifelse(...)` function is very helpful; this function takes a conditional statement as its first argument, followed by the value to assign if that statement is true, and the value to assign if that statement is false. `case_when(...)` is a development as it allows for assigning different values based on different conditions: *If A is true - do X, if B is true - do Y, if C is true...*

**Note:**
There are four terms in R which have a specific meaning and are always used without quotes:

- `NA` is the symbol for missing values in R (equivalent to STATA ".")
- `NULL` is the symbol for an empty slot (e.g. the default for optional function arguments is NULL)
- `TRUE` is for logical variables (equivalent of `1` in a STATA binary variable)
- `FALSE` is for logical variables (equivalent of `0` in a STATA binary variable)


```{r}
# Correct mistakes in age
cph$age[cph$age == 8] <- 18
cph$age[cph$age == 180] <- 18
cph$age[cph$age == 16 & cph$group == 0] <- 61
```

```{r}
# Correct mistake regarding onset date or a start hour
# Those who have no symptoms should also not have an onset date

cph <- cph %>% mutate(across(
  .cols = c(dayonset, starthour), 
  .fns = ~ ifelse(if_all(c(diarrhoea, bloody, vomiting), ~ . %in% c(0, NA)), 
                  NA, 
                  .)
  ))

```

```{r}

# Re-code sex to double -> why? why not factor?
cph <- cph %>% 
  mutate(sex = ifelse(sex == "male", 1, 0) %>% as.double())

```

```{r}

# Get an overview of dayonset and starthour
table(cph$dayonset, useNA = "always")
table(cph$starthour, useNA = "always")

# recode dayonset from character to time-format and create new variable that unites dayonset and starthour
cph <- cph %>% 
  mutate(dayonset = dmy(dayonset),
         # create new variable that combines dayonset AND starthour
         # 1. clean format of starthour using regex: consider doing this in the dataset and take out here for less confusion
         starthour = ifelse(str_detect(cph$starthour, "\\d{2}"), cph$starthour, paste0("0", cph$starthour)), 
         # 2. combine dayonset + starthour
         day_hour = paste0(as.character(dayonset), "-", starthour) %>% ymd_h()
         )
```


\pagebreak 

### Sub-task 03.03 Creating a case definition: 

You can create a new variable called 'case' which will have the value 1 if the person meets the case definition, otherwise 0 (for non-cases).  You can build a set of conditional statements to do this.  It is best to start with missing values, as for some people we don't have the information that we need to decide whether they are a case or not. 

```{r}

# create case definition 
cph <- cph %>% 
  mutate(case = case_when(
    if_any(c(diarrhoea, vomiting, bloody), ~ . == TRUE) ~ TRUE, # has any symptoms
    if_all(c(diarrhoea, vomiting, bloody), ~ . == FALSE) ~ FALSE, # has no symptoms
    day_hour < dmy_h("11.11.2006 18") ~ FALSE, # onset before
    day_hour > dmy_h("13.11.2006 18") ~ FALSE, # onset after
    meal == FALSE ~ NA, # not exposed to any food - could be a control though?!
    ))

```

We can then check how many cases and non-cases we have for our analysis, with the `table(...)` command.  There are a lot of missing values in the new 'case' variable; it might be worth considering assigning a `0` (i.e. code for non-case) to people who have no data for any symptoms (but think about the pros and cons of this assumption):

```{r}
# Tabulate the number of cases as a plausibility check
table(cph$case, useNA = "always")

# lots of NAs -why?
# Maybe symptoms-vars -> How many have no info on all symptoms?
filter(cph, if_all(c(diarrhoea, vomiting, bloody), ~ is.na(.))) %>% count(.)
# 135! Which of those have participated in the school dinner?
filter(cph, if_all(c(diarrhoea, vomiting, bloody), ~ is.na(.)) & meal == TRUE) %>% count(.)

# A recode to consider:
# cph$case[(is.na(cph$vomiting) | is.na(cph$diarrhoea) | is.na(cph$bloody)) & cph$meal == 1] <- 0

# We don´t have info on symptoms for 119 who participated in the school dinner -
# Seems reasonable to say that they probably did not develop symptoms - otherwise they would´ve told
cph <- cph %>% 
  mutate(
    case = ifelse(if_all(c(diarrhoea, vomiting, bloody), ~ is.na(.)) & meal == TRUE, FALSE, case))

```

Do a plausibility check to see if everything worked:

```{r}
# How many cases did you generate? 
table(cph$case, exclude = NULL)

# check if people were assigned properly:
# 1. anyone with symptoms, but not a case?
filter(cph, if_any(c(vomiting, diarrhoea, bloody), ~ . == 1 & case == FALSE))
# 2. anyone without symptoms but a case?
filter(cph, if_all(c(vomiting, diarrhoea, bloody), ~ . %in% c(0, NA) & case == TRUE))
```

For the sake of the anlysis, we exclude any people from the cohort who didn't eat at the dinner, because we specifically hypothesise a food item to be the vehicle of the outbreak. Excluding persons reduces the sample size and therefore the power slightly, but the investigators considered that this would increase specificity.

We can drop cases that do not meet the case definition by subsetting the data to those where a value for the `case` variable is not missing: 

```{r}
cph <- filter(cph, !is.na(case))
```


### Saving cleaned data 

You can save your cleaned dataset (and anything else that you have created in your workspace) as an R datafile (.Rda) using the `save()` command and re-load the same dataset using the `load()` command. 

In reality you would not usually do this unless your data set is very large; it should be sufficient to run your R script to import the raw data, clean it and analyse it in one go. 

If you wanted to, this is how you would do it:

```{r, eval= F}

# Save your dataset
save(cph, file = "cph.Rda")

# Load your dataset 
load("cph.Rda")

```

\pagebreak 

# 04. Developing an analysis plan

In the previous sessions we generated a dataset that we can directly use for the analyses we want to carry out in STATA. Ideally, you would want to create an analysis plan before data collection as it is an important way to ensure you collect all the data you need and that you use all the data you collect. However, often during outbreak investigations, you need to deploy questionnaires as soon as possible and before a plan has been developed.

In this session, before diving into any analyses, you will document what you intend to do with the data as it will help guide the descriptive and analytical analyses.

## Task 04. Develop an analysis plan and key dummy tables

In your analysis plan you should create a document to include:

 - research question(s) and / or hypotheses, if any
 - dataset(s) to be used
 - inclusion / exclusion criteria for records you will use in your analysis
 - list of variables that will be used in the main analysis (main exposure, outcome, stratifying variables)
 - statistical methods
 - key dummy tables* (univariable, bivariable, stratified) including title, labels and expected format for results.
 
* See STATA companion guide for example dummy tables.

\pagebreak 


# 05. Descriptive analysis in `R`

Before undertaking analytical epidemiology and statistical hypothesis testing, it is extremely important to use descriptive statistics. It guides us when it comes to planning the next steps and it helps to generate hypotheses about the pathogen and its source.


## Task 05. Descriptive analysis:

In this task, you will need to:

 - describe the data by person (age and sex, other attributes?)
 - describe the data by time (construct an epicurve)
 - create a histogram of the incubation period in 6 hour intervals
 - summarise the analysis so far and generate hypotheses:
     + Any surprising findings regarding cohort characteristics?
     + Is the shape of the epicurve in line with the hypothesis that the outbreak has a viral or toxic aetiology?
     + What other information can you obtain from the epicurve?
     + Does the summary of symptoms help you narrow down the suspected agent?
     + Do you have any hypotheses about the vehicle of infection so far?


\pagebreak

## Help for task 05:

### Loading the descriptive analysis data set:

In this section you will be using a dataset that has already been recoded and cleaned for you with the following code:


```{r}
# Read in your data from a csv file:

cph <- read.csv("Copenhagen_descriptive.csv", sep = ",", stringsAsFactors = FALSE) 

```



### Describe signs and symptoms of cases  

There are two possible ways of doing this, one is to go through each of the symptoms using the table command (version 1). The other is to use a *for-loop*, which repeats the procedure for each of the variables defined (version 2). 

The `cbind()` function attaches columns of datasets together. 

```{r}
# Version 1

# Create a frequency table of diarrhoea among cases
a <- table(cph$diarrhoea[cph$case == 1], cph$case[cph$case == 1])

# Frequencies divided by total cases (as proportion) and rounded to one decimal place

b <- cbind(a, round(prop.table(a)*100, digits = 1))

colnames(b) <- c("n", "%")

print(b)

# Note the percentages here are column totals
```

\pagebreak

```{r, output.lines = 12}
# Version 2

# Define the variables which you would like to look at and save as "vars"
vars <- c("diarrhoea","bloody","vomiting","abdo","nausea","fever","headache","jointpain")

# Start the for loop by saying, for all the variables saved in "vars" do:
for (var in vars){
  # Create a table for one variable at a time among cases and save under "b". 
  # using squarebrackets: [select row, select column]
  # select rows which are cases (cph$case==1)
  # select columns of interest (vars)
  b <- table(cph[cph$case == 1,var], cph[cph$case == 1,"case"])
  
  # Same as above to get proportions 
  b <- cbind(b,round(prop.table(b)*100, digits=1))
  
  # Rename the columns of your output using colnames function 
  colnames(b) <- c("n", "%")
  
  # Print the name of the variable as a header in the output
  print(var)
  
  # Print the table with frequency (column 1) and percentage (column)
  print(b)
  
  # Repeats from the beginning until all the variables have been looped through
}

```


\pagebreak

### Describe the cohort in terms of person 

We can first have a look at the age range: 

```{r}
#look at age and other variables of interest
summary(cph$age)
table(cph$age)

```


You may also wish to construct an age sex pyramid with the relevant function from the `EpiFunc` package.  Before constructing the figure, you will need to create an age group column with the `calc_agegroup()` function from this package.  (Hint: you may wish to alter the agegroup breaks if interested in seeing which class is most affected)


```{r}
# Create age group:
cph$agegroup <- EpiFunc::calc_agegroup(age = cph$age, breaks = c(0, 10, 16, 20, 50, Inf))

# Create label for sex:
cph$sexlab <- ifelse(cph$sex == 1, "male", "female")

# Create age sex pyramid:
agesex <- EpiFunc::age_sex_pyramid(data = cph, 
                                   age.grp.col = "agegroup", 
                                   sex.col = "sexlab", 
                                   col.pal = "phe", 
                                   blank.background = TRUE)

# Print the age sex pyramid:
agesex


```


### Calculate attack rates  

You can calculate attack rates (proportion ill) overall and stratified by different variables of interest, using the `prop.table()` function.  In this case you may wish to loop through a few different stratifying variables, which we can do using the same techniques as we did for creating tables of symptoms in the last section:

```{r}

# Students and sex
vars <- c("group","class", "sex")

#repeat for each of the variables in the list
for (var in vars){
  # create a basic two by two table
  # one variable at a time
  a <- table(cph[,var], cph$case)
  
  # create a table of proportions
  b <- round(prop.table(a, margin = 1) * 100, digits = 1)
  
  # bind the column with AR from b to the 2by2 table in a
  c <- cbind(a, b[,2])
  
  # rename your columns 
  colnames(c) <- c("non case", "case", "AR%")
  
  #print variable name to make output easier to understand
  print(var)
  print(c)
}

```


The same can be done with just a single variable: 

```{r}
# Total
# Create a basic table
  a <- table(cph$case)
  
  # Create a table of proportions
  b <- round(prop.table(a) * 100, digits = 1)
  
  #bind the column with AR from b to the 2by2 table in a
  total <- cbind(a, b)
  
  # Rename colnames
  colnames(total) <- c("n", "AR%")
  
  # Print the table
  print(total)

```


To see how to combine this code to create a ready-to-present table for exporting, see the appendix. 


\pagebreak 


### Creating epicurves (describing time):

To generate an epicurve, you need to generate a variable for the date of symptom onset which is in *date format*. The 
current variable `dayonset` is in character format.

```{r}
class(cph$dayonset)
```

Fortunately dates are easy to manipulate using the lubridate package, from Hadley Wickham's 'tidyverse'. There are a variety of functions for parsing dates by specifying the order in which the date components appear, for example `dmy()` for day-month-year, or `ymd()` for year-month-day. Note that some functions share names with others so you may have to specify the package you want by using a double colon, i.e. `package::function()`:

```{r}
# Convert the onset variable to date format:
cph$dayonset <- lubridate::dmy(cph$dayonset)

# Check that this has worked:
class(cph$dayonset)
```


```{r}

# We only want an onset date for cases though!
cph$dayonset[cph$case != 1] <- NA

```

**Construct an epicurve by day:**

It is possible to create an epicurve manually using base-R code (see appendix), however for simplicity, you can use the user written function from Daniel's `EpiFunc` package. 

The epicurve function allows creation of easily formatted epicurves. To find out more about the function, see the help file `?EpiFunc`.  This function uses the gpplot package (use `??ggplot2` to find out more about this package) and so is quite easily manipulated and saved after being created.

There are already many arguments (options) in the `epicurve()` function, which you can also explore if interested in displaying the data in a different way (see the function help file for more details).


```{r, warning = FALSE}
# Create the epicurve:
epicurveday <- EpiFunc::epicurve(x = cph,
                                 date.col = "dayonset",
                                 time.period = "day", 
                                 start.at = "2006-11-10", 
                                 stop.at = "2006-11-15",
                                 fill.by = "sexlab",
                                 fill.by.legend.title = "Sex",
                                 xlab = "Date of symptom onset", 
                                 ylab = "Count",
                                 angle = 90, 
                                 col.pal = "phe", 
                                 label.breaks = 0, 
                                 epi.squares = TRUE, 
                                 na.rm = TRUE, 
                                 blank.background = TRUE)

# Print the epicurve:
epicurveday



# As epicurveday is a ggplot object, it is possible to tailor it as desired:
# epicurveday <- epicurveday +
#                   # Rotating the x axis label by 90               
#                   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
#                   # Adding a title
#                   ggtitle("Gastroenteritis cases by date of onset, November 2006") +
#                    # Centring the title and reducing its size 
#                   theme(plot.title = element_text(hjust = 0.5, size = 11)) 
# 
# epicurveday


```


### Incubation period in hours

We need to create a variable which stores the time of the dinner. We can achive this using another lubridate function, `hours()`:

```{r}
# Create variable storing the time of the dinner:
cph$datedinner <- dmy(111106)

# Convert this to a date and time variable:
cph$datetimedinner <- cph$datedinner + hours(18)
```

To calculate the length of the incubation periods, we need to compare the time of dinner with the time of onset. 

```{r}
cph$datetimeonset <- cph$dayonset + hours(cph$starthour)
```

```{r}
cph$incubation <- cph$datetimeonset - cph$datetimedinner
```

Now we can look at the distribution of incubation times visually.

```{r}
ggplot(cph[!is.na(cph$incubation),]) +
  geom_histogram(aes(x = incubation), binwidth = 6) +
  scale_x_continuous()
```

### OPTIONAL TASK - Refining the epicurve (6 hour intervals)

We can construct a more refined epicurve using the datetimeonset variable

```{r, tidy= T}

# As a workaround to use the user written epicurve function 
# we can create a factor variable based on datetimeonset 
# and define the order of the levels. 

cph$timeonsetfactor <- factor(as.character(cph$datetimeonset), levels = c(
  "2006-11-11 21:00:00",
  "2006-11-12 03:00:00",
  "2006-11-12 09:00:00",
  "2006-11-12 15:00:00",
  "2006-11-12 21:00:00",
  "2006-11-13 03:00:00",
  "2006-11-13 09:00:00",
  "2006-11-13 15:00:00"
))

```


Using the epicurve function it is possible to choose what breakdown you would like on your time-axis by setting time.period to be "use.date.col.as.is". 


```{r}

hourlycurve <- EpiFunc::epicurve(x = cph[!is.na(cph$timeonsetfactor),], 
                                 date.col = "timeonsetfactor", 
                                 time.period = "use.date.col.as.is", 
                                 xlab = "Date and time of symptom onset", 
                                 ylab = "Count", 
                                 epi.squares = TRUE, na.rm = TRUE)


hourlycurve

```

\pagebreak


# 06. Univariable analysis (Cohort and case control)

To identify (the) potential vehicle(s) in the outbreak, proceed with an analytical study where you use statistical tests to investigate the associations of some suspicious food items with the disease.

## Task 06. Conducting a cohort study analysis:

In this task, you will need to:

 - Carry out a retrospective cohort study
 - Calculate appropriate measures of association and 95% confidence intervals for food exposures
 - Repeat the above steps as if this were a case control study (with same number of controls as cases)
 - Consider if you could also report these results in a cohort study setting?

In addition, interpret your results to answer the following questions:

 - Is there a relationship between the gender of the persons and being a case?
 - Is there a difference in class group between cases and non-cases?
 - Is there a difference between age in males and females?
 - Is there a dose-response relationship between the food items and being sick?
 
Hint: look at the food items that seem most suspicious to you!


## Help for task 06:

### Loading the relevant data set:

If you haven't completed the previous section then you can load the following dataset for the next steps.


```{r}
# Read in your data from a csv file: 

cph <- read.csv("Copenhagen_univariable.csv", sep = ",", stringsAsFactors = FALSE) 

```


There is more than one way to create univariable tables.  One gives you a two by two and quite a lot of info, and the other is user-written code which will provide you with nice output like STATA's cstable. 

Think about which variables you might want to include when checking for effect modification or confounding. One common strategy is to base this decision on the univariable results obtained and a p-value threshold of 0.25. Also, food items that are known risk factors for gastroenteritis could also be included regardless of their univariable p-value.


### Detailed output (method 1)

In order to use the epi.2by2 function, we first need to convert the outcome and exposure variables into factor variables to facilitate interpretation.  

Outcome and exposure variables of interest need to be factor variables prior to using the function, in order to be relevelled from (0,1) to (1,0) so that they can be correctly organised in 2-by-2 tables.

```{r}
# We list the outcome/exposure variables
vars <- c("shrimps", "veal", "pasta", "sauce", "champagne", "rocket", "case")


# Convert all of those variables to factor variables 
# and re-order the levels to aid interpretation:

for (var in vars) {
  cph[,var] <- factor(cph[,var],levels = c(1,0)) 
}
```


The `epi.2by2` function from the `epiR` package can be used to calculate both risk and odds ratios. You can find out more information on the function by writing `?epi.2by2` in the console. The epi.2by2 function requires data to be in a table format. We can specify that we want to calculate RRs or ORs by adding `method = "cohort.count"` or `method = "case.control"`, respectively. You can do that first for looking at the risk for being a case by exposure to shrimp in a cohort study design.  



```{r}
# Create a table with exposure and outcome variables
counts <- table(cph$shrimps, cph$case)

# Apply epi.2by2 function to the table
shrimp <- epiR::epi.2by2(counts, method = "cohort.count")

#to view the output
shrimp

```


This example involves using the `epi.2by2` function from the `epiR` package. To make things faster (but not necessarily easier to understand) it has been run in a for-loop and the output has been hidden because it is very long. 

With extra code you could extract information from `epi.2by2` and piece together an output table similar to STATA.
Here we have just chosen to show this in the console (extraction will be shown later in the case study).

```{r, results="hide"}

##vector of variables you want to run in the for-loop
vars <- c("pasta","veal", "champagne", "sauce", "shrimps")

#for each variable run a table and then get the rr out using epi.2by2 from the epiR package

for (var in vars) {
  # create table with the variable of interest by case classification
  a <- table(cph[,var], cph$case)
  
  # Pass your counts table to epi.2by2
  test <- epiR::epi.2by2(a,  method = "cohort.count")
  
  # Print the output of the table in the console and label which variable it is
  print(var)
  print(test)
}

```


### Simplified output (method 2)

A much more straight forward way is to simply apply a user-written function which requires much less input from you. 
The `sva` function (Daniel Gardner, PHE) basically integrates the steps from above to create a nice output table.  You have already sourced this function at the beggining of the case study; however if it is not in your environment (listed under 'functions' you can run that section of code again). 

To view the arguments (options) for this function, click on the function name in the Global environment panel in RStudio and you will be able to view some details.
 
```{r}
# Cohort study with risk ratios:
sva(cph, outcome = "case", exposures = c(vars), measure = "rr", verbose = TRUE)

```

\pagebreak

```{r}
# Case control study with odds ratios:
sva(cph, outcome = "case", exposures = c(vars), measure = "or", verbose = TRUE)

```


### Different statistical tests in `R`

Note that for the below tests, no tables are printed alongside, however you could create these tables using the `table()` and `prop.table()` (for percentages) functions. 


**01. Is gender associated with being a case?** 

```{r}

# using a chi-squared test
# chisq.test funciton requires you to input a table
chisq.test(table(cph$sex, cph$case))


# using a fisher's exact test
fisher.test(table(cph$sex, cph$case))


```



**02. Is class associated with being a case?**

Here you can either compare proportions, using the chi-squared test, or use the Wilcoxon-Mann-Whitney test to compare distributions. For the Wilcoxon-Mann-whitney test to work, all variables need to be numeric, however it is not possible to go directly from a factor to a numeric variable. The intermediate is to turn it in to a character (string) variable and then to a numeric variable. 

```{r}

# Using chi-squared
chisq.test(table(cph$class, cph$case))

# using the wilcoxon test 
# this is a ranksum test and the function calculates the counts from a line list

# change case from a factor to numeric
cph$case <- as.numeric(as.character(cph$case))

# run the wilcoxon 
wilcox.test(cph$class, cph$case)

```

**03. Is there a difference between age in males and females?** 

```{r}


# Using a wilcox test
wilcox.test(cph$age, cph$sex)

# Using a t-test to see difference in mean if normally distributed 
# shapiro tests if significantly different from normal distribution
shapiro.test(cph$age)

t.test(cph$age ~ cph$sex)


```




**04. Is there a dose-response relationship between the food items and being sick?** 

Look at the food items that seem most suspicious to you. Hint: it's pasta and veal!

There is two possible ways to do this. The first is to use the `wilcox.test()` function again, and the second is to calculate odds ratios using the `epi.2by2()` function. Using the `epi.2by2()` function for dose-responses gives more detailed results (traditionally this would be done using regression), so we have not shown it here; if you would like to see the code for this it is available in the appendix. 


```{r}


vars <- c("vealD", "pastaD")


for (var in vars){

  # Selects only those cases where dose response is filled out (not empty) and saves as a dataset
  dataset <- cph[is.na(cph[var]) == FALSE,]
  
  # saves in "form" as a text 
  # e.g. "pastaD ~ case" which can then be inserted in the wilcox.test to get dose response
  form <- as.formula(paste0(var,"~","case"))
  
  # put your formula above in to wilcox using the reduced dataset
  a <- wilcox.test(form, data = dataset)
  
  # show a table with doses and case and the wilcox output
  print(var)
  print(table(cph[,var], cph$case))
  print(a)
}

```

\pagebreak

# Part 07. Stratified analysis 


## Task 07. Performing stratified analysis:

You can see that those eating pasta or veal as well as drinking champagne have the highest risk of becoming ill. There are, however, many other food items that are associated with an increased risk (even if not statistically significant). At this stage we cannot conclude anything, but need to check for effect modification and confounding. This should be done by stratification.

## Help for task 07:

### Loading the data set:

If you haven't completed the previous section then you can load the following dataset for the next steps.


```{r}
# Read in your data from a csv file: 

cph <- read.csv("Copenhagen_stratified.csv", sep=",", stringsAsFactors = FALSE) 

```


Make sure that your variables of interest are factors in the correct order.  Outcome and exposure variables of interest need to be factor variables prior to using the function, in order to be relevelled from (0,1) to (1,0) so that they can be correctly organised in 2-by-2 tables. (We have already done this for our variables).

```{r}
# We list the outcome/exposure variables
vars <- c("shrimps", "veal", "pasta", "sauce", "champagne", "rocket", "case")


# Convert all of those variables to factors 
# and re-order the levels to aid interpretation:

for (var in vars) {
  cph[,var] <- factor(cph[,var],levels = c(1,0)) 
}

```


Stata users could use the `csinter` function to identify effect modifiers and confounders.  In `R`, the `epi.2by2()` function in the `epiR` package provides similar functionality. 


```{r}
# Make a 3-way table with exposure of interest, the outcome and the stratifying variable, in that order
a <- table(cph$veal, cph$case, cph$pasta)

# Use the epi.2by2 function to calculate RRs (by stating method = "cohort.count")
mhtable <- epi.2by2(a, method = "cohort.count")

# view the output
mhtable

```


You can then extract various outputs from the `epi2by2` table for piecing together your own output.  Here two levels of `$` signs are needed, as the function saves outputs to a list. 

```{r}

# Crude risk ratio:
mhtable$massoc$RR.crude.wald 

# Stratum-specific risk ratios:
mhtable$massoc$RR.strata.wald

# Adjusted risk ratio:
mhtable$massoc$RR.mh.wald

# You can combine all of those elements in to a single table using rbind
results <- rbind(mhtable$massoc$RR.crude.wald, 
                 mhtable$massoc$RR.strata.wald, 
                 mhtable$massoc$RR.mh.wald)


# We can label the rows of this table as below
rownames(results) <- c("Crude", "Strata 1", "Strata 0", "Adjusted")

# view output table:
results

```


You can then piece these parts together within a for-loop to investigate more variables: 


```{r}


# We list the exposure variables
vars <- c("veal", "rocket", "shrimps", "champagne", "sauce")

# Create an empty list to save the output of the loop
# A list is a collection of dataframes
outputs <- list()

# For each of the exposures of interest, run the stratified analysis from above
for (var in vars) {
  
  # Create 2x2 table:
  b <- table(cph[,var], cph$case, cph$pasta)
  
  # Run MH test on the table:
  mh <- epiR::epi.2by2(b, method = "cohort.count")
  
  # Bind the results together:
  resultstable <- rbind(mh$massoc$RR.crude.wald, 
                        mh$massoc$RR.strata.wald, 
                        mh$massoc$RR.mh.wald)
  
  # Create some row labels to aid interpretation:
  rownames(resultstable) <- c("Crude", "Strata 1", "Strata 0", "Adjusted")
  
  # Save your results table as a data.frame within your outputs list 
  outputs[[var]] <- resultstable
}

# view your results
outputs 

# Gives crude, stratum-specific and adjusted RRs

```


The exact same can be done for veal (switch veal and pasta):


```{r, eval = F}


# We list the exposure variables
vars <- c("pasta", "rocket", "shrimps", "champagne", "sauce")



# Create an empty list to save the output of the loop
outputs2 <- list()

# For each of the exposures of interest, run the stratified analysis from above
for (var in vars) {
  
  # Create 2x2 table:
  b <- table(cph[,var], cph$case, cph$veal)
  
  # Run MH test on the table:
  mh <- epi.2by2(b, method = "cohort.count")
  
  # Bind the results together:
  resultstable <- rbind(mh$massoc$RR.crude.wald, 
                        mh$massoc$RR.strata.wald, 
                        mh$massoc$RR.mh.wald)
  
  # Create row labels to aid interpretation:
  rownames(resultstable) <- c("Crude", "Strata 1", "Strata 0", "Adjusted")
  
  # save your output table as a data.frame within your list (of dataframes)
  outputs2[[var]] <- resultstable
}

# view your results
outputs2 

# Gives crude, stratum-specific and adjusted RRs

```

It appears that pasta confounds the association between eating veal and being a case. For a variable to be a confounder it needs to be associated both with the outcome (being a case) and with the exposure. We know from univariable analysis that pasta is associated with being a case; we can now check if it is also associated with veal.

```{r}
# Using a fisher's exact test:

fisher.test(table(cph$pasta, cph$veal))

```


### Integrate the analysis steps in one master script `R` file

This is easily done, if you have saved each of your analyses in a seperate script (e.g. one for cleaning, descriptive, univariable and stratified each), you can create a master script by simply using the `source()` function (as we did to read in user-written code) to run each of your scripts.  You can also separate your scripts into sections using multiple `###` symbols (you can then collapse and expand these sections in `RStudio`).

\pagebreak

# Appendix 


## Reading in datafiles to `R` from other formats (STATA and Microsoft Excel)

In `R` you can read datasets saved in STATA format (.dta), using a package called `haven`. 
See `?haven::read_dta` for details

```{r, echo=TRUE, eval=FALSE}

library(haven)

cph <- haven::read_dta("Copenhagen3.dta")

```


It is also possible to read in Microsoft Excel .xlsx files using a package called `openxlsx`. 

```{r, echo=TRUE, eval = FALSE}

cph <- openxlsx::read.xlsx("Copenhagen3.xlsx", sheet = 1)


```


## Optional task - Export your output to Microsoft Excel 

This section produces a full descriptive table and exports it to Microsoft Excel.  This is just to illustrate that you can create tables however you would like to. If you are feeling confident then take a look at the code; if not just skip it!

```{r, echo=TRUE, eval=FALSE}

a <- matrix(NA, ncol = 4, nrow = 8)

rownames(a) <- c("student","teacher", "1", "2", "3", "male", "female", "total")

colnames(a) <- c("Number", "% of Total", "Number of cases", "Attack rate (in %)")

for (var in vars){
  
  b <- table(cph[,var], cph$case)
  
  catTotal <- rowSums(b)
  
  denom <- sum(b)
  
  props <- round(catTotal/denom * 100, digits = 1)
  
  cases <- b[,2]
  
  AR <- round(cases/catTotal *100, digits = 1)
  
  a[rownames(a) %in% names(catTotal), "Number"]  <- catTotal
  a[rownames(a) %in% names(props), "% of Total"]  <- props
  a[rownames(a) %in% names(cases), "Number of cases"]  <- cases
  a[rownames(a) %in% names(AR), "Attack rate (in %)"]  <- AR
}



a

z <- data.frame(unclass(a))

write.xlsx(z, "My_table.xlsx")


```


## Manual epicurve construction using base plotting functions

Creating an epicurve using the builtin `barplot()` function is one option, as it is relatively easy to define the time axis - though getting boxes per case is a bit of a hassle.  Note: several lines of code have been turned to comments (#png(...) and #dev.off()), these pieces of code are used to save the plot as a picture file on your desktop, however you can run the code in your R browser without saving the picture output to a folder. 



```{r, tidy=TRUE, eval = FALSE}

# Convert dayonset to date:
cph$dayonset <- lubridate::dmy(cph$dayonset)

# Table to use in plotting 
count <- table(cph$dayonset)

# Define parameters to use in the image file (.png)
# after plotting (see ?png for help)
aa <- 2
h1 = 1800*aa
w1 = sqrt(2)*h1
r1 = 300
ps1 <- 20

# Define where and what you want to save your output picture file:
# png(filename = "epicurve.png", 
#     width = w1, 
#     height = h1, 
#     res = r1, 
#     pointsize = ps1)

# Regular bar plot: 
epicurve <- barplot(count, 
                    beside = TRUE, 
                    axes = FALSE, 
                    xaxt = "n", 
                    xlab = NULL, 
                    legend.text = NULL, 
                    col = "red", 
                    space = c(0,0))

# Axis numbering 
axis(2, at = seq(0, max(count), by = 10), tick = TRUE, pos = 0, las = 1, cex = 1.5)

axis(1, at = c(epicurve), labels = rownames(count), tick = TRUE, pos = 0)

# Another way of doing axis labels 
mtext(text = "Cases (n)", 
      side = 2,
      line = 3,
      cex = 1.5)

# This command urns off the plotting function and saves the output picture
# dev.off()

```


## Creating time levels with coding rather than typing it out 

```{r, echo=TRUE, eval=FALSE}

a <- expand.grid(c("0","6","12", "18"), paste0("2006-11-", 10:14))

b <- paste0(a$Var2," ", a$Var1)

cph$starttime <- factor(cph$starttime, levels = b)

```


## Using epi.2by2 for investigating dose-response relationships


Using `epi.2by2()` to get odds ratios for a dose response relationship is a bit more difficult than what we did above, as you need to patch together tables to feed in to the function for each dose-level.  Here you need case to be a factor again.  (It would of course be much easier to use regression for this, but this demonstrates some data handling techniques).



```{r}

# Change case back to a factor 
cph$case <- factor(cph$case,levels = c(1,0)) 


# Create your table of counts 
counts <- table(cph$pastaD, cph$case) 

# create an empty matrix to fill with your output
# this matrix should be the same number of rows as your counts dataset
# you also want 3 columns to put your output in to:
output <- matrix(NA, nrow = nrow(counts), ncol = 3)

# For each of the dose response levels
for (i in 2:nrow(counts)) {
  
  # Create your 2by2 counts table by combining the ref group with one dose level (row)
  input <- rbind(counts[1,], counts[i,])
  
  # Feed this table to epi2by2
  test <- epi.2by2(input,  method="case.control")
  
  # Over-write the row of your output matrix with the relevant estimates
  output[i,] <- as.numeric(test$massoc$OR.strata.wald)

}

# Name the rows of your output accordingly
colnames(output) <- c("OR", "lower", "upper") 


# Bind your output table to your counts table
counts <- cbind(counts, output)

```



\pagebreak 

# Conclusions of the case study 


## Descriptive analyses 

We didn't find anything surprising in the descriptive analysis of the cohort's age given it consists of two groups, the students and the teachers.

The distribution of the cohort regarding sex, group and class also didn't reveal anything unusual. Students seem a bit more affected by the outbreak than teachers and the attack rate is higher for older students in higher classes. This, however, is a purely descriptive result.

When constructing an epicurve, we need to decide on the resolution, i.e. the time interval for a single bar. A rule of thumb is to use one third or one fourth of the average incubation period as an interval. For our investigation this means we should use approximately a 6h interval.

This seems a good choice indeed as we saw that the daily interval was too coarse to really see the signal we're after. The epicurve and the summary of the incubation period show that there seemed to be a rapid onset of symptoms following exposure. This is in line with our previous suspicion that a virus or a toxin might be the causative agent in the outbreak.

The unimodal shape with the sharp peak suggests a point source, while the tail on the right hand side could be explained by secondary cases or background noise. Also people that only consumed a little contaminated food and therefore only a low infectious dose could have a longer incubation period and could explain the late cases.
The above results are in line with norovirus as the prime suspect, but the symptoms are not a textbook fit. There are too few people that experienced vomiting!


## Univariate analyses 
The interesting results here are that the two food items that are most suspicious are pasta and veal. In particular, the dose response relationship that was found for pasta points towards pasta as the potential vehicle. Pasta as such is unlikely to be contaminated, but as you can see in the label, it was served with pesto!
Before you jump to conclusions, be aware that this result could be due to confounding! Maybe pasta was clean but eaten by all the people who ate the food item that actually was contaminated!

## Stratified analyses 
We found that pasta consumption confounds the association between eating veal and being ill. The crude (univariable) result for veal suggests that veal is a risk factor (crude RR = 1.52, CI: 1.00-2.31), but when we adjust for the consumption of pasta we see that actually this result is due to the fact that most people who ate veal also ate pasta.
Within the stratum of the people who ate pasta, veal has no effect (RR = 1.19, CI: 0.59-2.40). The same holds within the stratum of people who didn't eat pasta (RR = 1.05, CI = 0.38, 2.92). This is why the adjusted MH-RR also suggests that veal has no effect (RRadj = 1.14, CI: 0.64-2.03).
This result taken together with the dose response relationship we found earlier for pasta gives additional evidence that there was something going on with the pesto!

## Conclusions of the investigation 
In summary, it is fair to say that there was both epidemiological and microbiological evidence that the pasta salad with pesto was the most likely vehicle of transmission in this outbreak. Further investigations focused on how the pasta salad with pesto could have become contaminated and on lessons learned from this outbreak that could then communicated both the scientific community, the caterer and the general public.


